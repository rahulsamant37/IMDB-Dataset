{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['MLFLOW_TRACKING_URI']=\"https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow\"\n",
    "os.environ['MLFLOW_TRACKING_USERNAME']=\"rahulsamantcoc2\"\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD']=\"33607bcb15d4e7a7cca29f0f443d16762cc15549\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\IMDB-Dataset\\\\notebook'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\IMDB-Dataset'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any\n",
    "\n",
    "@dataclass\n",
    "class ModelEvalConfig:\n",
    "    root_dir: str\n",
    "    test_data_path: str\n",
    "    metric_file_name: str\n",
    "    models: Dict[str, Dict[str, str]]\n",
    "    all_params: dict\n",
    "    target_column: str\n",
    "    mlflow_uri: str\n",
    "    model_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import read_yaml, create_directories,save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_eval_config(self, model_type: str) -> ModelEvalConfig:\n",
    "\n",
    "        config = self.config.model_evaluation\n",
    "        params=self.params.model_params[model_type]\n",
    "        schema = self.schema.TARGET_COLUMN\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_eval_config = ModelEvalConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            metric_file_name=config.metric_file_name,\n",
    "            models=config.models,\n",
    "            all_params=params,\n",
    "            target_column=schema.name,\n",
    "            mlflow_uri=\"https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow\",\n",
    "            model_type=model_type\n",
    "        )\n",
    "        return model_eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from urllib.parse import urlparse\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from src import logger\n",
    "from typing import List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEval:\n",
    "    def __init__(self, config: ModelEvalConfig):\n",
    "        self.config = config\n",
    "        self.vocab_size = 10000\n",
    "        self.best_model_metrics = None\n",
    "        self.best_model_type = None\n",
    "\n",
    "    def preprocess_features(self, data: pd.DataFrame) -> np.ndarray:\n",
    "        features_list = []\n",
    "        for _, row in data.iterrows():\n",
    "            feature_str = row.iloc[0]\n",
    "            if isinstance(feature_str, str):\n",
    "                features = np.array([\n",
    "                    min(int(x), self.vocab_size - 1) \n",
    "                    for x in feature_str.strip('[]').split()\n",
    "                ])\n",
    "            else:\n",
    "                features = np.minimum(np.array(feature_str), self.vocab_size - 1)\n",
    "            features_list.append(features)\n",
    "        return np.vstack(features_list)\n",
    "\n",
    "    def Eval(self, actual, pred):\n",
    "        pred_binary = (pred >= 0.5).astype(int)\n",
    "        actual_binary = actual.astype(int)\n",
    "        \n",
    "        accuracy = accuracy_score(actual_binary, pred_binary)\n",
    "        precision = precision_score(actual_binary, pred_binary)\n",
    "        recall = recall_score(actual_binary, pred_binary)\n",
    "        f1 = f1_score(actual_binary, pred_binary)\n",
    "        auc = roc_auc_score(actual_binary, pred)\n",
    "        cm = confusion_matrix(actual_binary, pred_binary)\n",
    "        return accuracy, precision, recall, f1, auc, cm\n",
    "\n",
    "    def update_best_model(self, metrics, model_type):\n",
    "        accuracy, _, _, f1, auc, _ = metrics\n",
    "        current_score = (accuracy + f1 + auc) / 3  # Combined metric\n",
    "        \n",
    "        if self.best_model_metrics is None or current_score > self.best_model_metrics['combined_score']:\n",
    "            self.best_model_metrics = {\n",
    "                'model_type': model_type,\n",
    "                'accuracy': accuracy,\n",
    "                'f1': f1,\n",
    "                'auc': auc,\n",
    "                'combined_score': current_score\n",
    "            }\n",
    "            self.best_model_type = model_type\n",
    "\n",
    "    def log_into_mlflow(self):\n",
    "        test_data = pd.read_csv(self.config.test_data_path)\n",
    "        model_path = self.config.models.get(self.config.model_type, {}).get('model_path')\n",
    "        \n",
    "        if model_path is None:\n",
    "            raise ValueError(f\"Model path not found for model type: {self.config.model_type}\")\n",
    "            \n",
    "        model = joblib.load(model_path)\n",
    "        test_x = self.preprocess_features(test_data.drop([self.config.target_column], axis=1))\n",
    "        test_y = test_data[self.config.target_column].values\n",
    "\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            predicted_qualities = model.predict(test_x)\n",
    "            metrics = self.Eval(test_y, predicted_qualities)\n",
    "            self.update_best_model(metrics, self.config.model_type)\n",
    "            \n",
    "            accuracy, precision, recall, f1, auc, cm = metrics\n",
    "            scores = {\n",
    "                \"accuracy\": float(accuracy),\n",
    "                \"precision\": float(precision),\n",
    "                \"recall\": float(recall),\n",
    "                \"f1\": float(f1),\n",
    "                \"auc\": float(auc),\n",
    "                \"confusion_matrix\": cm.tolist()\n",
    "            }\n",
    "            \n",
    "            save_json(path=Path(self.config.metric_file_name), data=scores)\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            \n",
    "            for metric_name, value in scores.items():\n",
    "                if metric_name != \"confusion_matrix\":\n",
    "                    mlflow.log_metric(metric_name, value)\n",
    "            \n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"IMDB\")\n",
    "            else:\n",
    "                mlflow.sklearn.log_model(model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-12-25 23:28:37,206: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-12-25 23:28:37,388: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-12-25 23:28:37,410: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-12-25 23:28:37,423: INFO: common: created directory at: artifacts]\n",
      "[2024-12-25 23:28:37,473: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 125ms/step\n",
      "[2024-12-25 23:28:57,654: INFO: common: Binary file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/25 23:30:24 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Successfully registered model 'IMDB'.\n",
      "2024/12/25 23:30:38 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: IMDB, version 1\n",
      "Created version '1' of model 'IMDB'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run industrious-auk-17 at: https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow/#/experiments/0/runs/2560f8b1951e47e191f1012617600d6b\n",
      "üß™ View experiment at: https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow/#/experiments/0\n",
      "[2024-12-25 23:30:38,984: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 39ms/step\n",
      "[2024-12-25 23:30:43,453: INFO: common: Binary file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/25 23:31:29 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'IMDB' already exists. Creating a new version of this model...\n",
      "2024/12/25 23:31:41 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: IMDB, version 2\n",
      "Created version '2' of model 'IMDB'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run handsome-zebra-157 at: https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow/#/experiments/0/runs/b2f8b994ef29477a80c1bc503d1aeb8b\n",
      "üß™ View experiment at: https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow/#/experiments/0\n",
      "[2024-12-25 23:31:42,686: INFO: common: created directory at: artifacts/model_evaluation]\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 74ms/step\n",
      "[2024-12-25 23:31:47,993: INFO: common: Binary file saved at: artifacts\\model_evaluation\\metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/25 23:32:41 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'IMDB' already exists. Creating a new version of this model...\n",
      "2024/12/25 23:32:54 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: IMDB, version 3\n",
      "Created version '3' of model 'IMDB'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run auspicious-swan-301 at: https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow/#/experiments/0/runs/b58df84619b344859edf90e13b6a1b2d\n",
      "üß™ View experiment at: https://dagshub.com/rahulsamantcoc2/IMDB-Dataset.mlflow/#/experiments/0\n",
      "\n",
      "Best Model: rnn\n",
      "Metrics: Accuracy=0.5220, F1=0.5230, AUC=0.5116\n"
     ]
    }
   ],
   "source": [
    "best_model_metrics = None\n",
    "model_evals = []\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    \n",
    "    for model_type in ['rnn', 'lstm', 'gru']:\n",
    "        model_eval_config = config.get_model_eval_config(model_type=model_type)\n",
    "        model_eval = ModelEval(config=model_eval_config)\n",
    "        model_eval.log_into_mlflow()\n",
    "        model_evals.append(model_eval)\n",
    "    \n",
    "    # Find best model\n",
    "    best_model = max(model_evals, key=lambda x: x.best_model_metrics['combined_score'])\n",
    "    print(f\"\\nBest Model: {best_model.best_model_type}\")\n",
    "    print(f\"Metrics: Accuracy={best_model.best_model_metrics['accuracy']:.4f}, \"\n",
    "          f\"F1={best_model.best_model_metrics['f1']:.4f}, \"\n",
    "          f\"AUC={best_model.best_model_metrics['auc']:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
